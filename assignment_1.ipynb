{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 472 Assignment 1\n",
    "## Team: RoboCops\n",
    "### Team Members:\n",
    "- Rongxi Meng (40045067)\n",
    "- Chen Qian (27867808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1: Load the Dataset in Python\n",
    "In this section, we will load the dataset and prepare it for the machine learning models.\n",
    "\n",
    "### 2.1(a) Preprocessing the Penguin Dataset\n",
    "The Penguin dataset contains string features 'island' and 'sex' that need to be converted to a numerical format suitable for MLP models. We will explore two methods for this conversion:\n",
    "- **Method i**: Convert these features into 1-hot vectors (also known as dummy-coded data).\n",
    "- **Method ii**: Manually convert these features into numerical categories.\n",
    "\n",
    "### 2.1(b) Assessing the Abalone Dataset\n",
    "We need to determine if the Abalone dataset can be used in its current form. If it contains features similar to the Penguin dataset that need conversion, we will apply the two methods mentioned above to transform any string features into a numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "penguins_data = pd.read_csv(\"./datasets/penguins.csv\")\n",
    "abalone_data = pd.read_csv(\"./datasets/abalone.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of each dataset to verify\n",
    "penguins_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For penguins_data\n",
    "penguins_data_encoded = pd.get_dummies(penguins_data, columns=['island', 'sex'], drop_first=False)\n",
    "\n",
    "# Convert boolean values to integers (0 and 1) for the one-hot encoded columns\n",
    "encoded_columns = [col for col in penguins_data_encoded.columns if 'Type_' in col]\n",
    "penguins_data_encoded[encoded_columns] = penguins_data_encoded[encoded_columns].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Type' is the original column with 'M', 'F', 'I' values for sex\n",
    "# This column will be the target variable, so we don't include it in the one-hot encoding process\n",
    "\n",
    "# Separate the features and the target variable\n",
    "x_abalone = abalone_data.drop('Type', axis=1)  # Features\n",
    "y_abalone = abalone_data['Type']  # Target variable\n",
    "\n",
    "# Encode the target variable 'Type'\n",
    "abalone_data_encoded = pd.get_dummies(y_abalone, drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows to verify\n",
    "print(penguins_data_encoded.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows to verify\n",
    "print(penguins_data_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_abalone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_abalone.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows to verify\n",
    "print(abalone_data_encoded.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows to verify\n",
    "abalone_data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Plot the percentage of instances in each output class\n",
    "- Store the graphic in a file called `penguin-classes.gif` / `abalone-classes.gif`.\n",
    "- This analysis of the dataset will allow you to determine if the classes are balanced.\n",
    "- Decide which metric is more appropriate to evaluate the performance.\n",
    "- Be prepared to discuss this at the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution for 'species' in penguins_data\n",
    "# Then use bash to convert the PNG to GIF in folder \"result\": magick convert penguin-classes.png penguin-classes.gif\n",
    "species_counts = penguins_data['species'].value_counts(normalize=True) * 100\n",
    "species_counts.plot(kind='bar', color='skyblue', figsize=(8, 6))\n",
    "plt.title('Distribution of Species in Penguins Dataset')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.xlabel('Species')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#plt.savefig('./result/penguin-classes.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution for 'Type' in abalone_data\n",
    "# Then use bash to convert the PNG to GIF in folder \"result\": magick convert abalone-classes.png penguin-classes.gif\n",
    "type_counts = abalone_data['Type'].value_counts(normalize=True) * 100\n",
    "type_counts.plot(kind='bar', color='coral', figsize=(8, 6))\n",
    "plt.title('Distribution of Types in Abalone Dataset')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.xlabel('Type')\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#plt.savefig('./result/abalone-classes.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Dataset Splitting\n",
    "- Utilize the `train_test_split` function with default parameters to divide the dataset into training and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_penguins = penguins_data_encoded.drop('species', axis=1)  # Features (excluding the target column 'species')\n",
    "y_penguins = penguins_data_encoded['species']  # Target column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Training and Testing Classifiers\n",
    "This section involves the training and evaluation of four distinct classifiers. Each classifier will be assessed based on its performance metrics, and the results will be documented accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the model's performance.\n",
    "    \n",
    "    Returns:\n",
    "    - cm: Confusion matrix\n",
    "    - precision, recall, f1: Precision, recall, and F1-measure for each class\n",
    "    - accuracy: Overall accuracy\n",
    "    - macro_f1: Macro-average F1\n",
    "    - weighted_f1: Weighted-average F1\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(x_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return cm, precision, recall, f1, accuracy, macro_f1, weighted_f1\n",
    "\n",
    "\n",
    "def append_performance_to_file(filename, model_name, best_params, cm, precision, recall, f1, accuracy, macro_f1, weighted_f1):\n",
    "    \"\"\"\n",
    "    Append the model's performance metrics to a file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(\"***** \" + model_name + \" *****\\n\")\n",
    "        if best_params:\n",
    "            f.write(\"Best Parameters: \" + str(best_params) + \"\\n\")\n",
    "        f.write(\"(B) Confusion Matrix:\\n\")\n",
    "        f.write(str(cm) + \"\\n\")\n",
    "        f.write(\"(C) Precision, Recall, F1-measure for each class:\\n\")\n",
    "        for i, (p, r, f1_score) in enumerate(zip(precision, recall, f1)):\n",
    "            f.write(f\"Class {i}: Precision={p:.2f}, Recall={r:.2f}, F1={f1_score:.2f}\\n\")\n",
    "        f.write(f\"(D) Accuracy: {accuracy:.2f}, Macro-average F1: {macro_f1:.2f}, Weighted-average F1: {weighted_f1:.2f}\\n\")\n",
    "        f.write(\"*************************\\n\\n\")\n",
    "\n",
    "\n",
    "def run_model_multiple_times(model, x, y, num_runs=5):\n",
    "    accuracies = []\n",
    "    macro_f1s = []\n",
    "    weighted_f1s = []\n",
    "\n",
    "    for _ in range(num_runs):\n",
    "        # Split the data in each iteration\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "        # Print the first 5 labels of y_test for each iteration\n",
    "        print(f\"Run {_+1}:\")\n",
    "        print(y_test.head())\n",
    "\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "         # Predict and calculate metrics\n",
    "        y_pred = model.predict(x_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "        macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        macro_f1s.append(macro_f1)\n",
    "        weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        weighted_f1s.append(weighted_f1)\n",
    "\n",
    "        # Print out the metrics for this run\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        print(f\"Macro F1: {macro_f1:.2f}\")\n",
    "        print(f\"Weighted F1: {weighted_f1:.2f}\")\n",
    "        print(\"----------------------------\")\n",
    "\n",
    "    # Calculate average and variance for each metric\n",
    "    results = {\n",
    "        'avg_accuracy': np.mean(accuracies),\n",
    "        'var_accuracy': np.var(accuracies),\n",
    "        'avg_macro_f1': np.mean(macro_f1s),\n",
    "        'var_macro_f1': np.var(macro_f1s),\n",
    "        'avg_weighted_f1': np.mean(weighted_f1s),\n",
    "        'var_weighted_f1': np.var(weighted_f1s)\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_and_save_results(model, x, y, filename, model_name, best_params=None, num_runs=5):\n",
    "    \"\"\"\n",
    "    Evaluate the model's performance, save the results to a file, and run the model multiple times to compute average and variance of metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained model to evaluate\n",
    "    - x: Features of the dataset\n",
    "    - y: Target labels of the dataset\n",
    "    - filename: Name of the file to save results\n",
    "    - model_name: Name of the model (e.g., \"Base-DT\")\n",
    "    - best_params: Best parameters (if any) for the model\n",
    "    - num_runs: Number of times to run the model for computing average and variance\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "    \n",
    "    # Evaluate the performance of the model on the test data\n",
    "    cm, precision, recall, f1, accuracy, macro_f1, weighted_f1 = evaluate_model_performance(model, x_test, y_test)\n",
    "\n",
    "    # Append the performance metrics to the file\n",
    "    append_performance_to_file(filename, model_name, best_params, cm, precision, recall, f1, accuracy, macro_f1, weighted_f1)\n",
    "\n",
    "    # Run the model multiple times and compute average and variance of metrics\n",
    "    results = run_model_multiple_times(model, x, y, num_runs)\n",
    "\n",
    "    # Append the average and variance results to the file\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(f\"***** {model_name} Multiple Runs ({num_runs} times) *****\\n\")\n",
    "        f.write(f\"(A) Average Accuracy: {results['avg_accuracy']:.2f}, Variance: {results['var_accuracy']:.2f}\\n\")\n",
    "        f.write(f\"(B) Average Macro F1: {results['avg_macro_f1']:.2f}, Variance: {results['var_macro_f1']:.2f}\\n\")\n",
    "        f.write(f\"(C) Average Weighted F1: {results['avg_weighted_f1']:.2f}, Variance: {results['var_weighted_f1']:.2f}\\n\")\n",
    "        f.write(\"*************************\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Base-DT: Decision Tree with Default Parameters\n",
    "- Illustrate the decision tree graphically.\n",
    "- For the abalone dataset, you may limit the tree depth for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_visualize_base_dt(x_train, y_train, feature_names, title, save_path, max_depth=None):\n",
    "    \"\"\"\n",
    "    Train a base Decision Tree classifier with an optional maximum depth and visualize the tree.\n",
    "    \n",
    "    Parameters:\n",
    "    - x_train: Training data features\n",
    "    - y_train: Training data labels\n",
    "    - feature_names: Names of the features in x_train\n",
    "    - title: Title for the visualization\n",
    "    - save_path: Path to save the visualization\n",
    "    - max_depth: Optional maximum depth of the tree\n",
    "    \n",
    "    Returns:\n",
    "    - base_dt: Trained Decision Tree classifier\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the Decision Tree classifier with optional max_depth\n",
    "    base_dt = DecisionTreeClassifier(max_depth=max_depth)\n",
    "\n",
    "    # Train the classifier\n",
    "    base_dt.fit(x_train, y_train)\n",
    "\n",
    "    # Convert class names to strings\n",
    "    class_names_str = base_dt.classes_.astype(str)\n",
    "\n",
    "    # Visualize the Decision Tree\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plot_tree(base_dt, filled=True, feature_names=feature_names, class_names=class_names_str, rounded=True)\n",
    "    plt.title(title)\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    \n",
    "    return base_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "base_dt_penguins = train_and_visualize_base_dt(x_penguins, y_penguins, x_penguins.columns, \"Base Decision Tree for Penguins Data\", './result/penguins_base_decision_tree.png',max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "base_dt_abalone = train_and_visualize_base_dt(x_abalone, y_abalone, x_abalone.columns, \"Base Decision Tree for Abalone Data\", './result/abalone_base_decision_tree.png',max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result & txt for penguin model\n",
    "evaluate_and_save_results(base_dt_penguins, x_penguins, y_penguins, \"./result/penguin-performance.txt\", \"Base-DT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result & txt for abalone model\n",
    "evaluate_and_save_results(base_dt_abalone, x_abalone, y_abalone, \"./result/abalone-performance.txt\", \"Base-DT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Top-DT: Optimized Decision Tree via Grid Search\n",
    "- Utilize grid search to find the best-performing Decision Tree based on the evaluation function established in step (3).\n",
    "- Experiment with the following hyper-parameters:\n",
    "  - Criterion: `gini` or `entropy`\n",
    "  - Max depth: Choose two specific values or `None`\n",
    "  - Min samples split: Select three specific values\n",
    "- Graphically represent the decision tree.\n",
    "- For the abalone dataset, consider limiting the tree depth for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_visualize_top_dt(x_train, y_train, feature_names, param_grid, title, save_path):\n",
    "    \"\"\"\n",
    "    Perform a grid search to find the best Decision Tree classifier, train it, and visualize the tree.\n",
    "    \n",
    "    Parameters:\n",
    "    - x_train: Training data features\n",
    "    - y_train: Training data labels\n",
    "    - feature_names: Names of the features in x_train\n",
    "    - param_grid: Hyperparameters and their possible values for grid search\n",
    "    - title: Title for the visualization\n",
    "    - save_path: Path to save the visualization\n",
    "    \n",
    "    Returns:\n",
    "    - best_dt: Best Decision Tree classifier from the grid search\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the Decision Tree classifier\n",
    "    dt = DecisionTreeClassifier()\n",
    "    \n",
    "    # Set up the grid search\n",
    "    grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n",
    "    \n",
    "    # Train the grid search on the training data\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    \n",
    "    # Extract the best Decision Tree model\n",
    "    best_dt = grid_search.best_estimator_\n",
    "    \n",
    "    # Visualize the best Decision Tree\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plot_tree(best_dt, filled=True, feature_names=feature_names, class_names=best_dt.classes_, rounded=True)\n",
    "    plt.title(title)\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    \n",
    "    return best_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "top_dt_penguins = train_and_visualize_top_dt(x_penguins, y_penguins, x_penguins.columns, param_grid, \"Top Decision Tree for Penguins Data\", './result/penguins_top_decision_tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function\n",
    "top_dt_abalone = train_and_visualize_top_dt(x_abalone, y_abalone, x_abalone.columns, param_grid, \"Top Decision Tree for Abalone Data\", './result/abalone_top_decision_tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_save_results(top_dt_penguins, x_penguins, y_penguins, \"./result/penguin-performance.txt\", \"Top-DT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_save_results(top_dt_abalone, x_abalone, y_abalone, \"./result/abalone-performance.txt\", \"Top-DT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Base-MLP: Basic Multi-Layered Perceptron\n",
    "- Construct a Multi-Layered Perceptron (MLP) with the following specifications:\n",
    "  - Two hidden layers, each with 100 neurons.\n",
    "  - Activation function: Sigmoid/Logistic.\n",
    "  - Solver: Stochastic Gradient Descent (SGD).\n",
    "  - Default values for all other parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base_mlp(x_train, y_train):\n",
    "    \"\"\"\n",
    "    Train a base Multi-Layered Perceptron (MLP) with the specified parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - x_train: Training data features\n",
    "    - y_train: Training data labels\n",
    "    \n",
    "    Returns:\n",
    "    - base_mlp: Trained MLP classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the MLP classifier with the specified parameters\n",
    "    base_mlp = MLPClassifier(hidden_layer_sizes=(100, 100), \n",
    "                             activation='logistic', \n",
    "                             solver='sgd', \n",
    "                             random_state=42)\n",
    "    \n",
    "    # Train the classifier\n",
    "    base_mlp.fit(x_train, y_train)\n",
    "    \n",
    "    return base_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_mlp_penguins = train_base_mlp(x_penguins, y_penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Base-MLP model's performance, save the results to the file, \n",
    "# and run the model multiple times to compute average and variance of metrics\n",
    "evaluate_and_save_results(base_mlp_penguins, x_penguins, y_penguins, \"./result/penguin-performance.txt\", \"Base-MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_mlp_abalone = train_base_mlp(x_abalone, y_abalone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Base-MLP model's performance, save the results to the file, \n",
    "# and run the model multiple times to compute average and variance of metrics\n",
    "evaluate_and_save_results(base_mlp_abalone, x_abalone, y_abalone, \"./result/abalone-performance.txt\", \"Base-MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Top-MLP: Enhanced Multi-Layered Perceptron via Grid Search\n",
    "- Implement a grid search to find a high-performing Multi-Layered Perceptron (MLP) model. The grid search should explore the following hyperparameters:\n",
    "  - Activation functions to consider: `sigmoid`, `tanh`, and `relu`.\n",
    "  - Network architectures: Choose two configurations, such as:\n",
    "    - Two hidden layers with 30 and 50 nodes respectively.\n",
    "    - Three hidden layers with 10 nodes each.\n",
    "  - Solvers: `adam` and `stochastic gradient descent`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters and their possible values\n",
    "param_grid_mlp = {\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'hidden_layer_sizes': [(30, 50), (10, 10, 10)],\n",
    "    'solver': ['adam', 'sgd']\n",
    "}\n",
    "\n",
    "# Initialize the MLP classifier\n",
    "mlp = MLPClassifier(max_iter=1000)  # Setting max_iter to a higher value for convergence\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_mlp = GridSearchCV(mlp, param_grid_mlp, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search_mlp.fit(x_penguins, y_penguins)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_mlp = grid_search_mlp.best_params_\n",
    "print(\"Best Parameters for Top-MLP:\", best_params_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the MLP with the best hyperparameters\n",
    "top_mlp = MLPClassifier(**best_params_mlp, max_iter=1000)\n",
    "top_mlp.fit(x_penguins, y_penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_save_results(top_mlp, x_penguins, y_penguins, \"./result/penguin-performance.txt\", \"Top-MLP\", best_params=best_params_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "grid_search_mlp.fit(x_abalone, y_abalone)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_mlp = grid_search_mlp.best_params_\n",
    "print(\"Best Parameters for Top-MLP:\", best_params_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the MLP with the best hyperparameters\n",
    "top_mlp = MLPClassifier(**best_params_mlp, max_iter=1000)\n",
    "top_mlp.fit(x_abalone, y_abalone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_save_results(top_mlp, x_abalone, y_abalone, \"./result/abalone-performance.txt\", \"Top-MLP\", best_params=best_params_mlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
